# -*- coding: utf-8 -*-
"""FastAPI_testing

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yKGHSJHQCZsbsER25ZLC9w5NCEs-G3Pj
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import google.generativeai as genai
import asyncio
import time

app = FastAPI()
clients = {}

def get_gemini_client(api_key: str):
    if api_key not in clients:
        clients[api_key] = genai.Client(api_key=api_key)
    return clients[api_key]

async def call_gemini_async(query, previous_conversation, gender, username, botname, bot_prompt, llm_api_key_string, language):
    try:
        language_instruction = f"Respond in {language} language in 2 or 3 lines only unless longer answers are expected."
        full_prompt = (
            f"{bot_prompt}\n"
            f"{language_instruction}\n"
            f"Previous conversation: {previous_conversation[-1000:]}\n"
            f"{username}: {query}\n"
            f"{botname}:"
        )
        
        client = get_gemini_client(llm_api_key_string)
        
        def make_api_call():
            response_text = ""
            for chunk in client.models.generate_content_stream(
                model="gemini-2.0-flash",
                contents=[full_prompt]
            ):
                if chunk.text:
                    response_text += chunk.text
            return response_text
        
        loop = asyncio.get_event_loop()
        response_text = await loop.run_in_executor(None, make_api_call)
        
        response_raw = response_text
        for old, new in [("User1", username), ("user1", username), ("[user1]", botname), ("[User1]", botname)]:
            response_raw = response_raw.replace(old, new)
            
        return response_raw.strip()
        
    except Exception as e:
        return f"Error: {str(e)}"

class ChatRequest(BaseModel):
    query: str
    previous_conversation: str = ""
    gender: str = "female"
    username: str = "Anushka"
    botname: str = "Assistant"
    bot_prompt: str = ""
    llm_api_key_string: str
    language: str = "English"

async def chat(request: ChatRequest):
    try:
        start = time.time()
        response = await call_gemini_async(
            request.query,
            request.previous_conversation,
            request.gender,
            request.username,
            request.botname,
            request.bot_prompt,
            request.llm_api_key_string,
            request.language
        )
        end = time.time()
        latency = (end - start) * 1000
        return {"answer": response, "latency_ms": latency}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
